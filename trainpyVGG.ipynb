{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "from torch.autograd import Variable\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "import time\n",
    "import math\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "content = None\n",
    "content_weight = 1e2\n",
    "style = None\n",
    "style_weight = 1\n",
    "epochs = 20000\n",
    "print_interval = 1000\n",
    "plot_interval = 1000\n",
    "output = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_content (257, 1249)\n",
      "a_content_torch torch.Size([1, 3, 257, 1249])\n",
      "sr 22050\n",
      "------------\n",
      "a_style (257, 1249)\n",
      "a_style_torch torch.Size([1, 3, 257, 244])\n",
      "sr 22050\n"
     ]
    }
   ],
   "source": [
    "# WAV TO SPECTRUM BLOCK\n",
    "CONTENT_FILENAME = \"boy18.wav\"\n",
    "STYLE_FILENAME = \"girl52.wav\"\n",
    "\n",
    "a_content, src = wav2spectrum(CONTENT_FILENAME)\n",
    "a_style, srs = wav2spectrum(STYLE_FILENAME)\n",
    "\n",
    "a_content_torch = torch.from_numpy(a_content)[None, None, :, :] \n",
    "a_style_torch = torch.from_numpy(a_style)[None, None, :, :]\n",
    "\n",
    "a_content_torch = a_content_torch.repeat(1, 3, 1, 1)  # [batch_size, 3, height, width]\n",
    "a_style_torch = a_style_torch.repeat(1, 3, 1, 1)      # [batch_size, 3, height, width]\n",
    "\n",
    "print(\"a_content\", a_content.shape)\n",
    "print(\"a_content_torch\", a_content_torch.shape)\n",
    "\n",
    "print(\"sr\",src )\n",
    "print(\"------------\")\n",
    "print(\"a_style\", a_content.shape)\n",
    "print(\"a_style_torch\", a_style_torch.shape)\n",
    "\n",
    "print(\"sr\",src )\n",
    "\n",
    "# Output\n",
    "# a_content (257, 244)\n",
    "# a_content_torch torch.Size([1, 1, 257, 244])\n",
    "# sr 22050\n",
    "# ------------\n",
    "# a_style (257, 244)\n",
    "# a_style_torch torch.Size([1, 1, 257, 355])\n",
    "# sr 22050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dando\\AppData\\Local\\Temp\\ipykernel_30548\\1072698933.py:8: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Display the Content spectrum\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(a_content, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label=\"Log Amplitude\")\n",
    "plt.title(\"Spectrogram of the Audio\")\n",
    "plt.xlabel(\"Time Frames\")\n",
    "plt.ylabel(\"Frequency Bins\")\n",
    "plt.show()\n",
    "plt.savefig(\"Content_Spectrum.png\")  # Save the plot as an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dando\\AppData\\Local\\Temp\\ipykernel_30548\\3122756499.py:8: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Display the Style spectrum\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(a_style, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label=\"Log Amplitude\")\n",
    "plt.title(\"Spectrogram of the Audio\")\n",
    "plt.xlabel(\"Time Frames\")\n",
    "plt.ylabel(\"Frequency Bins\")\n",
    "plt.show()\n",
    "plt.savefig(\"Style_Spectrum.png\")  # Save the plot as an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 257, 1249])\n",
      "torch.Size([1, 3, 257, 244])\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    a_content_torch = a_content_torch.cuda()\n",
    "    print(a_content_torch.shape)\n",
    "if cuda:\n",
    "    a_style_torch = a_style_torch.cuda()\n",
    "    print(a_style_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the VGG-based feature extractor\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, selected_layers):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Load pretrained VGG19 model\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "        # Select layers to extract features from\n",
    "        self.selected_layers = selected_layers\n",
    "        self.vgg_layers = nn.ModuleList([vgg[i] for i in range(max(selected_layers) + 1)])\n",
    "        \n",
    "        # Freeze VGG weights to prevent updates during training\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            x = layer(x)\n",
    "            if i in self.selected_layers:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dando\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dando\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGGFeatureExtractor(\n",
       "  (vgg_layers): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define selected layers to extract features from\n",
    "# Example: Use features from layers corresponding to relu1_2, relu2_2, relu3_4, etc.\n",
    "selected_layers = [3, 8, 17, 26, 35]\n",
    "\n",
    "# Initialize the feature extractor\n",
    "model = VGGFeatureExtractor(selected_layers)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 -> a_C: torch.Size([1, 64, 257, 244]), a_S: torch.Size([1, 64, 257, 355]), a_G: torch.Size([1, 64, 257, 244])\n",
      "Layer 8 -> a_C: torch.Size([1, 128, 128, 122]), a_S: torch.Size([1, 128, 128, 177]), a_G: torch.Size([1, 128, 128, 122])\n",
      "Layer 17 -> a_C: torch.Size([1, 256, 64, 61]), a_S: torch.Size([1, 256, 64, 88]), a_G: torch.Size([1, 256, 64, 61])\n",
      "Layer 26 -> a_C: torch.Size([1, 512, 32, 30]), a_S: torch.Size([1, 512, 32, 44]), a_G: torch.Size([1, 512, 32, 30])\n",
      "Layer 35 -> a_C: torch.Size([1, 512, 16, 15]), a_S: torch.Size([1, 512, 16, 22]), a_G: torch.Size([1, 512, 16, 15])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example: Generated, content, and style tensors\n",
    "a_C_var = a_content_torch.float()  # Content tensor\n",
    "a_S_var = a_style_torch.float()    # Style tensor\n",
    "a_G_var = torch.randn_like(a_C_var).float()  # Random initialized tensor for generated\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    a_C_var = a_C_var.cuda()\n",
    "    a_S_var = a_S_var.cuda()\n",
    "    a_G_var = a_G_var.cuda()\n",
    "\n",
    "# Extract features\n",
    "a_C = model(a_C_var)  # Content features\n",
    "a_S = model(a_S_var)  # Style features\n",
    "a_G = model(a_G_var)  # Generated features\n",
    "\n",
    "# Print feature shapes\n",
    "for i, (c, s, g) in enumerate(zip(a_C, a_S, a_G)):\n",
    "    print(f\"Layer {selected_layers[i]} -> a_C: {c.shape}, a_S: {s.shape}, a_G: {g.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a_C = a_C[0]\n",
    "a_S = a_S[0]\n",
    "a_G = a_G[0]\n",
    "\n",
    "\n",
    "type(a_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated spectrogram (initialized with small random noise)\n",
    "a_G_var = torch.randn(a_content_torch.shape) * 1e-3\n",
    "if cuda:\n",
    "    a_G_var = a_G_var.cuda()\n",
    "\n",
    "a_G_var.requires_grad = True\n",
    "\n",
    "# Optimizer for the generated spectrogram\n",
    "optimizer = torch.optim.Adam([a_G_var])\n",
    "\n",
    "# Coefficients for content and style loss\n",
    "style_param = style_weight\n",
    "content_param = content_weight\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = epochs\n",
    "print_every = print_interval\n",
    "plot_every = plot_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss tracking\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "# Utility function to compute elapsed time\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{m}m {s:.2f}s'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 5.00% 1m 56.45s content_loss: 12.2053 style_loss: 259.8648 total_loss: 272.0702\n",
      "2000 10.00% 3m 53.39s content_loss: 13.4224 style_loss: 133.0526 total_loss: 146.4751\n",
      "3000 15.00% 5m 48.96s content_loss: 13.7781 style_loss: 80.7092 total_loss: 94.4873\n",
      "4000 20.00% 7m 43.67s content_loss: 13.7278 style_loss: 46.9445 total_loss: 60.6722\n",
      "5000 25.00% 10m 27.30s content_loss: 13.4987 style_loss: 27.0963 total_loss: 40.5950\n",
      "6000 30.00% 12m 38.86s content_loss: 13.2103 style_loss: 18.0150 total_loss: 31.2253\n",
      "7000 35.00% 14m 32.59s content_loss: 12.9721 style_loss: 14.5875 total_loss: 27.5596\n",
      "8000 40.00% 16m 26.28s content_loss: 12.8311 style_loss: 13.3538 total_loss: 26.1849\n",
      "9000 45.00% 18m 19.96s content_loss: 12.7459 style_loss: 12.7828 total_loss: 25.5287\n",
      "10000 50.00% 20m 13.67s content_loss: 12.6887 style_loss: 12.3966 total_loss: 25.0853\n",
      "11000 55.00% 22m 8.82s content_loss: 12.6469 style_loss: 12.1267 total_loss: 24.7736\n",
      "12000 60.00% 24m 3.01s content_loss: 12.6146 style_loss: 11.9112 total_loss: 24.5258\n",
      "13000 65.00% 25m 57.39s content_loss: 12.5909 style_loss: 11.7448 total_loss: 24.3357\n",
      "14000 70.00% 27m 51.32s content_loss: 12.5798 style_loss: 11.6049 total_loss: 24.1847\n",
      "15000 75.00% 29m 45.20s content_loss: 12.5723 style_loss: 11.5082 total_loss: 24.0805\n",
      "16000 80.00% 31m 38.98s content_loss: 12.5653 style_loss: 11.4420 total_loss: 24.0073\n",
      "17000 85.00% 33m 33.38s content_loss: 12.5610 style_loss: 11.3908 total_loss: 23.9518\n",
      "18000 90.00% 35m 27.59s content_loss: 12.5571 style_loss: 11.3475 total_loss: 23.9046\n",
      "19000 95.00% 37m 21.87s content_loss: 12.5542 style_loss: 11.3083 total_loss: 23.8625\n",
      "20000 100.00% 39m 16.52s content_loss: 12.5509 style_loss: 11.2779 total_loss: 23.8288\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def trainModel(optimizer, current_loss):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        a_G = model(a_G_var)\n",
    "        a_G = a_G[0]\n",
    "\n",
    "        # Compute content and style losses\n",
    "        content_loss = content_param * compute_content_loss(a_C, a_G)\n",
    "        style_loss = style_param * compute_layer_style_loss(a_S, a_G)\n",
    "\n",
    "        # Total loss\n",
    "        loss = content_loss + style_loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"{epoch} {epoch / num_epochs * 100:.2f}% {timeSince(start)} \"\n",
    "                  f\"content_loss: {content_loss.item():.4f} \"\n",
    "                  f\"style_loss: {style_loss.item():.4f} \"\n",
    "                  f\"total_loss: {loss.item():.4f}\")\n",
    "            current_loss += loss.item()\n",
    "\n",
    "        # Update loss list for plotting\n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "\n",
    "    return\n",
    "\n",
    "# Start training\n",
    "start = time.time()\n",
    "trainModel(optimizer, current_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 257, 244])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_G_var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting output to Wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (3, 257, 244)\n",
      "p (3, 257, 244)\n",
      "x.shape: (31104,)\n",
      "type(x)): <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def librosa_write(outfile, x, sr):\n",
    "    if version.parse(librosa.__version__) < version.parse('0.8.0'):\n",
    "        librosa.output.write_wav(outfile, x, sr)\n",
    "    else:\n",
    "        soundfile.write(outfile, x, sr)\n",
    "\n",
    "\n",
    "def spectrum2wav(spectrum, sr, outfile):\n",
    "    # Return the all-zero vector with the same shape of `a_content`\n",
    "    a = np.exp(spectrum) - 1\n",
    "    p = 2 * np.pi * np.random.random_sample(spectrum.shape) - np.pi\n",
    "    \n",
    "    print(\"a\", a.shape)\n",
    "    print(\"p\", p.shape)\n",
    "\n",
    "    for i in range(50):\n",
    "        S = a * np.exp(1j * p)\n",
    "        x = librosa.istft(S)\n",
    "        p = np.angle(librosa.stft(x, n_fft=N_FFT))\n",
    "\n",
    "    x = x.mean(axis=0)  # Mix down to mono\n",
    "    # x = x.flatten()\n",
    "    # print(\"x:\",x)\n",
    "    print(\"x.shape:\",x.shape)\n",
    "    print(\"type(x)):\",type(x))\n",
    "\n",
    "    librosa_write(outfile, x, sr)\n",
    "\n",
    "\n",
    "gen_spectrum = a_G_var.cpu().data.numpy().squeeze()\n",
    "gen_audio_C = output + \".wav\"\n",
    "spectrum2wav(gen_spectrum, src, gen_audio_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 257, 244])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a_G_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 257, 244)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_spectrum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.savefig('loss_curve.png')\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "# we then use the 2nd column.\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Content Spectrum\")\n",
    "plt.imsave('Content_Spectrum.png', a_content[:400, :])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "# we then use the 2nd column.\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Style Spectrum\")\n",
    "plt.imsave('Style_Spectrum.png', a_style[:400, :])\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "# we then use the 2nd column.\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"CNN Voice Transfer Result\")\n",
    "plt.imsave('VGGGen_Spectrum.png', gen_spectrum[0][:400, :])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
